{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent\n",
    "import requests\n",
    "import os\n",
    "import os.path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_scrap(link, category):\n",
    "    key = '1783319260d70f84da21868ce0fd6207'\n",
    "\n",
    "    payload = {'api_key': key, 'url': link, 'keep_headers': 'true'}\n",
    "    head = {\n",
    "        'authority': 'api.tracker.gg',\n",
    "        'method': 'GET',\n",
    "        'path': '/api/v1/valorant/lfg/search?region=any&playlist=competitive&skill=any',\n",
    "        'scheme': 'https',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'nb-NO,nb;q=0.9,no;q=0.8,en-US;q=0.7,en;q=0.6,ru;q=0.5',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'cookie': '__cflb=02DiuFQAkRrzD1P1mdkJhfdTc9AmTWwYkL5dhc6BaWz7E; X-Mapping-Server=s14; __cf_bm=eCW52EohIqvzfugOyKvNvo4spvffbOwnBy5YZBvh4gQ-1653777883-0-AYz5PvgOpqpL7/UCjWN7/dokINYUw/EV1D4T6eY5n9YecMF2lY0M4TWWdDyySEalQ3lzxua3IYwD9KUjzZtmFdM=',\n",
    "        'sec-ch-ua': 'Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': 'macOS',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36'\n",
    "        }\n",
    "    count = 0\n",
    "    path = f'excel_dir/{category}.csv'\n",
    "    if not os.path.exists(path):\n",
    "        while True:\n",
    "            response = requests.get('http://api.scraperapi.com', params=payload, headers=head)\n",
    "            if response.status_code == 200:\n",
    "                print(f'Success:',response.status_code)\n",
    "                return response\n",
    "            else:\n",
    "                print(f'Failure:',response.status_code)\n",
    "                count += 1\n",
    "                if count >= 2:\n",
    "                    return None\n",
    "                continue\n",
    "    else:\n",
    "        print(f'\\t{category} has already been scrapped')\n",
    "        return None\n",
    "#200\tSuccessful response. - ScraperAPI\n",
    "#400    Bad request is sent when the server cannot understand the request sent by the client. - Requests\n",
    "#401    Unauthorized is sent whenever fulfilling the requests requires supplying valid credentials. - Requests\n",
    "#403    Forbidden means that the server understood the request but will not fulfill it. - Requests\n",
    "#404\tPage requested does not exist. - ScraperAPI\n",
    "#410\tPage requested is no longer available. - ScraperAPI\n",
    "#500\tAfter retrying for 60 seconds, the API was unable to receive a successful response. - ScraperAPI\n",
    "#429\tYou are sending requests too fast, and exceeding your concurrency limit. - ScraperAPI\n",
    "#403\tYou have used up all your API credits. - ScraperAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(response, category):\n",
    "# Write html file from data scrapped\n",
    "    with open(f\"html_dir/{category}.html\",\"w\",encoding=\"utf-8\")as html_file:\n",
    "        html_file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(category):    \n",
    "    # Read html file from scrapped data\n",
    "    with open(f\"html_dir/{category}.html\",\"r\",encoding='utf-8')as html_file:\n",
    "        output=html_file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(output, 'lxml')\n",
    "        \n",
    "        products = soup.find_all('article', class_ = 'ads__unit')\n",
    "            \n",
    "        product_dict = {}\n",
    "        for product in products:\n",
    "            pass\n",
    "            image = product.find('img', class_= 'img-format__img')['src']\n",
    "            link = product.find('a', class_ = 'ads__unit__link')['href']\n",
    "            \n",
    "            price = product.find('div', class_= 'ads__unit__img__ratio__price').text\n",
    "            if price == 'Til salgs' or price == 'Gis bort' or price == 'Ønskes kjøpt':\n",
    "                price = None\n",
    "            else:\n",
    "                price = price.replace('kr', '')\n",
    "                price = int(''.join(char for char in price if char.isalnum()))\n",
    "\n",
    "            payed = product.find('span', class_ = 'status status--sponsored u-mb8')\n",
    "            name = product.find('a', class_ = 'ads__unit__link').text\n",
    "            location = None\n",
    "            age = None\n",
    "            if payed == None:\n",
    "                age = product.find('div', class_ = 'ads__unit__content__details').div.text\n",
    "                location = product.find('div', class_ = 'ads__unit__content__details')\n",
    "                location = location.find_next()\n",
    "                location = location.find_next().text\n",
    "\n",
    "            itemnumber = product.find('a', class_ = 'ads__unit__link')['id']\n",
    "            product_dict[itemnumber] = [name, price, age, location, image, link, 'finn', category]       \n",
    "      \n",
    "        \n",
    "        return product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finn_scrap(links):\n",
    "\n",
    "    # Check if we have the correct directories\n",
    "    if not os.path.exists('html_dir'):\n",
    "        os.mkdir('html_dir')\n",
    "    if not os.path.exists('excel_dir'):\n",
    "        os.mkdir('excel_dir')\n",
    "\n",
    "    for link in links:\n",
    "        # Category on the item in our current link\n",
    "        part_1 = link.split('=')[2].split('&')[0].replace('+', '_')\n",
    "        #part_2 = link.split('=')[1].split('&')[0]\n",
    "        part_2 = link.split('page=')[1].split('&')[0]\n",
    "        category = part_1 + '_' + part_2\n",
    "\n",
    "        print(category)\n",
    "        # Write down the data we get from scrapping\n",
    "        scrapped = do_scrap(link, category)\n",
    "        if scrapped != None:\n",
    "            print('\\tScrap: Success!')\n",
    "            print('\\tWriting...')\n",
    "            write(scrapped, category)\n",
    "            print('\\tReading...')\n",
    "            current = read(category)\n",
    "            # Used for columns in excel\n",
    "            col = ['name', 'price', 'age', 'location', 'image', 'link', 'store', 'category']\n",
    "            # Create dataframe\n",
    "            df = pd.DataFrame(current.values(), columns = col)\n",
    "            #df = df[df['name'].apply(lambda x: '1070' in x)]\n",
    "            #df = df.loc[df['price']<4000]\n",
    "            \n",
    "            df.to_csv(f'excel_dir/{category}.csv')\n",
    "            print(f'\\tScrapped: {category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "pages = 9\n",
    "for x in range(1,pages+1):\n",
    "        links.append(f'https://www.finn.no/bap/forsale/search.html?location=0.22030&location=0.22054&page={x}&product_category=2.93.3907.79&sort=RELEVANCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.22054_1\n",
      "\t0.22054_1 has already been scrapped\n",
      "2\n",
      "0.22054_2\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_2\n",
      "3\n",
      "0.22054_3\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_3\n",
      "4\n",
      "0.22054_4\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_4\n",
      "5\n",
      "0.22054_5\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_5\n",
      "6\n",
      "0.22054_6\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_6\n",
      "7\n",
      "0.22054_7\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_7\n",
      "8\n",
      "0.22054_8\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_8\n",
      "9\n",
      "0.22054_9\n",
      "Success: 200\n",
      "\tScrap: Success!\n",
      "\tWriting...\n",
      "\tReading...\n",
      "\tScrapped: 0.22054_9\n"
     ]
    }
   ],
   "source": [
    "finn_scrap(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines all pages into one file\n",
    "\n",
    "#os.chdir(\"excel_dir\")\n",
    "\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob(f'excel_dir/*.{extension}')]\n",
    "\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
