{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import html_to_json\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(link, category, tries):\n",
    "    # Save page\n",
    "    key = '1783319260d70f84da21868ce0fd6207'\n",
    "    api_link = f\"http://api.scraperapi.com?api_key={key}&url={link}\"\n",
    "    if 'https://' in link:\n",
    "        link.replace('https://', '')\n",
    "    while True:\n",
    "        try:\n",
    "            page=urlopen(api_link)\n",
    "            return page\n",
    "        except HTTPError:\n",
    "            print('\\tSleeping...')\n",
    "            time.sleep(15)\n",
    "            tries+=1\n",
    "            print(f'Scrapping: {category}, Try: {tries}')\n",
    "            scrap(link, category, tries)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(page):\n",
    "    # Read data on page\n",
    "    html_bytes=page.read()\n",
    "    # Decode data from page\n",
    "    html=html_bytes.decode(\"utf-8\")\n",
    "    # Write html file from data scrapped\n",
    "    with open(f\"html_dir/{category}.html\",\"w\",encoding=\"utf-8\")as html_file:\n",
    "        html_file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(category):    \n",
    "    # Read html file from scrapped data\n",
    "    \n",
    "    with open(f\"html_dir/{category}.html\",\"r\",encoding='utf-8')as html_file:\n",
    "        output=html_file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(output, 'lxml')\n",
    "        if category == 'skjermer':\n",
    "            products = soup.find_all('div', class_ = 'product-list-item subscription-price-visible')\n",
    "        else:\n",
    "            products = soup.find_all('div', class_ = 'product-list-item')\n",
    "            \n",
    "        product_dict = {}\n",
    "        for product in products:\n",
    "            link = str('komplett.no' + product.a['href'])\n",
    "            image = str('komplett.no/img/p/800/' + product.a['href'].split('/')[2])\n",
    "            sale = None\n",
    "            price_now = product.find('span', class_= 'product-price-now').text\n",
    "            price_now = int(''.join(char for char in price_now if char.isalnum()))\n",
    "            price_before = product.find('div', class_= 'product-price-before')\n",
    "            if price_before != None:\n",
    "                price_before = product.find('div', class_= 'product-price-before').text.replace('Før', '').replace(',-', '').strip()\n",
    "                price_before = int(''.join(char for char in price_before if char.isalnum()))\n",
    "                sale = True\n",
    "            \n",
    "            name = product.h2.text.replace('å', 'aa').replace('ø', 'o')\n",
    "            stats = product.p.text.replace(',','').split()\n",
    "            available = product.find('span', class_='stockstatus-stock-details').text.replace('å', 'aa').replace('ø', 'o')\n",
    "            itemnumber = product.find('div', class_='product-data').text.replace(' ', '').replace('\\n', '').split('/')[0].split(':')[1]\n",
    "\n",
    "            product_dict[itemnumber] = [name, price_now, price_before, sale, stats, available, image, link, 'komplett']       \n",
    "        \n",
    "        return product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links = ['https://www.komplett.no/category/11157/tv-lyd-bilde/tv-video/tv-er?nlevel=10719%C2%A730000%C2%A711157&hits=240',\n",
    "        'www.komplett.no/category/11158/datautstyr/skjermer/skjermer?nlevel=10000%C2%A710392%C2%A711158&hits=264',\n",
    "        'www.komplett.no/category/21635/gaming/gaming-utstyr/gaming-tastatur?nlevel=10431%C2%A721603%C2%A721635&hits=168',\n",
    "        'www.komplett.no/category/199889/hjem-fritid/stoevsugere-rengjoering?nlevel=10560%C2%A7199889&hits=120']\n",
    "single_link = [links[1]]\n",
    "\n",
    "for link in links:\n",
    "    tries = 0\n",
    "    \n",
    "    # Category on the item in our current link\n",
    "    category = link.split('?')[0].split('/')[-1].replace('-','_')\n",
    "    print(category)\n",
    "    # Write down the data we get from scrapping\n",
    "    write(scrap(link, category, tries))\n",
    "    current = read(category)\n",
    "    # Used for columns in excel\n",
    "    col = ['name', 'price_now', 'price_before', 'sale', 'stats', 'available', 'image', 'link', 'site']\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(current.values(), columns = col)\n",
    "    # Convert dataframe to excel file\n",
    "    df.to_csv(f'excel_dir/{category}.csv')\n",
    "    print(f'scrapped: {category}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f38b465b4ade997603fa1c81d4c5b7ecf3a62a523ec595fe3d6f4efcae3a4f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
