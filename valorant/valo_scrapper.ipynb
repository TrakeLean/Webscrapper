{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(link):\n",
    "    if 'https://' in link:\n",
    "        link = link.replace('https://', '')\n",
    "    if 'http://' in link:\n",
    "        link = link.replace('http://', '')\n",
    "        \n",
    "    # scrap\n",
    "    key = '1783319260d70f84da21868ce0fd6207'\n",
    "    api_link = f\"http://api.scraperapi.com?api_key={key}&url={link}\"\n",
    "    page=urlopen(api_link)\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(page, category):\n",
    "    # Read data on page\n",
    "    html_bytes=page.read()\n",
    "    # Decode data from page\n",
    "    html=html_bytes.decode(\"utf-8\")\n",
    "    # Write html file from data scrapped\n",
    "    with open(f\"html_dir/{category}.html\",\"w\",encoding=\"utf-8\")as html_file:\n",
    "        html_file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(category):    \n",
    "    # Read html file from scrapped data\n",
    "    with open(f\"html_dir/{category}.html\",\"r\",encoding='utf-8')as html_file:\n",
    "        output=html_file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(output, 'html')\n",
    "        print(\"_________________________________________________\")\n",
    "        players = soup.find_all('tr', class_ = 'data-v-6a4dcc8d')\n",
    "        for player in players:\n",
    "            print(player)\n",
    "       \n",
    "        # product_dict = {}\n",
    "        # for product in products:\n",
    "        #     link = str('komplett.no' + product.a['href'])\n",
    "        #     image = str('komplett.no/img/p/800/' + product.a['href'].split('/')[2] + '.jpg')\n",
    "        #     sale = None\n",
    "        #     price_now = product.find('span', class_= 'product-price-now').text\n",
    "        #     price_now = int(''.join(char for char in price_now if char.isalnum()))\n",
    "        #     price_before = product.find('div', class_= 'product-price-before')\n",
    "        #     if price_before != None:\n",
    "        #         price_before = product.find('div', class_= 'product-price-before').text.replace('Før', '').replace(',-', '').strip()\n",
    "        #         price_before = int(''.join(char for char in price_before if char.isalnum()))\n",
    "        #         sale = True\n",
    "            \n",
    "        #     name = product.h2.text.replace('å', 'aa').replace('ø', 'o')\n",
    "        #     stats = product.p.text.replace(',','').split()\n",
    "        #     #available = product.find('span', class_='stockstatus-stock-details').text.replace('å', 'aa').replace('ø', 'o').split(' ')[0]\n",
    "        #     try:\n",
    "        #         available = product.find('span', class_='stockstatus-stock-details').text.replace('å', 'aa').replace('ø', 'o').split(' ')[0].replace('+', '')\n",
    "        #         if available == 'Ikke' or available == 'Bestillingsvare.':\n",
    "        #             available = 0\n",
    "        #     except AttributeError as error:\n",
    "        #             available = error\n",
    "                    \n",
    "        #     itemnumber = product.find('div', class_='product-data').text.replace(' ', '').replace('\\n', '').split('/')[0].split(':')[1]\n",
    "\n",
    "        #     product_dict[itemnumber] = [name, price_now, price_before, sale, available, stats, image, link, 'komplett', category]       \n",
    "        \n",
    "        # return product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def komplett_scrap(mode):\n",
    "\n",
    "    # Check if we have the correct directories\n",
    "    if not os.path.exists('html_dir'):\n",
    "        os.mkdir('html_dir')\n",
    "    if not os.path.exists('excel_dir'):\n",
    "        os.mkdir('excel_dir')\n",
    "\n",
    "    link = f\"https://tracker.gg/valorant/{mode}\"\n",
    "    scrapped = scrap(link)\n",
    "    write(scrapped, mode)\n",
    "    read(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "komplett_scrap(\"lfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Category on the item in our current link\n",
    "        # category = link.split('?')[0].split('/')[-1].replace('-','_')\n",
    "        # print(category)\n",
    "        # # Write down the data we get from scrapping\n",
    "        # scrapped = scrap(link, category, tries)\n",
    "        # if scrapped != None:\n",
    "        #     print('\\tScrap: Success!')\n",
    "        #     print('\\tWriting...')\n",
    "        #     write(scrapped, category)\n",
    "        #     print('\\tReading...')\n",
    "        #     current = read(category)\n",
    "        #     # Used for columns in excel\n",
    "        #     col = ['name', 'price_now', 'price_before', 'sale', 'available', 'stats', 'image', 'link', 'site', 'category']\n",
    "        #     # Create dataframe\n",
    "        #     df = pd.DataFrame(current.values(), columns = col)\n",
    "        #     #df = df.loc[df['sale']==True]\n",
    "        #     # Convert dataframe to excel file\n",
    "        #     df.to_csv(f'excel_dir/{category}.csv')\n",
    "        #     print(f'\\tScrapped: {category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e64607258e25fce727d4ce894b48231697c899ba80c79879091fc5b7d23fa9c1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
